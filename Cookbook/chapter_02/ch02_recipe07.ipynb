{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Large Datasets Efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Handling large datasets efficiently in Python can be achieved by in different ways.\n",
    "In this recipe we will focus on strategies to optimize to optimize performance and memory usage when managing data with `pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it\n",
    "1. Import the library `pandas` as `pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing  Data by Chunks\n",
    "2. Read the data in chunks to avoid loading the entire dataset into memory at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100  # Adjust based on available memory\n",
    "chunks = pd.read_csv('data/customers-10000.csv', chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use a generator to iterate over the data chunks and process them without loading everything in memory at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n",
      "I am processing a chunk\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(\"I am processing a chunk\")\n",
    "    # Some process for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combine the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise the Data Types\n",
    "\n",
    "Using appropriate data types to reduce memory usage. \n",
    "\n",
    "1. Check the data types of your `DataFrame` with the method `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                 int64\n",
       "Customer Id          object\n",
       "First Name           object\n",
       "Last Name            object\n",
       "Company              object\n",
       "City                 object\n",
       "Country              object\n",
       "Phone 1              object\n",
       "Phone 2              object\n",
       "Email                object\n",
       "Subscription Date    object\n",
       "Website              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Downcast numeric columns. For example `float32` instead of `float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert `object` columns to categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900                     Puerto Rico\n",
       "9901              Russian Federation\n",
       "9902                          Tuvalu\n",
       "9903                         Comoros\n",
       "9904                       Argentina\n",
       "                    ...             \n",
       "9995                   Cote d'Ivoire\n",
       "9996                         Namibia\n",
       "9997    United States Virgin Islands\n",
       "9998                           Niger\n",
       "9999                         Tunisia\n",
       "Name: Country, Length: 100, dtype: category\n",
       "Categories (88, object): ['Afghanistan', 'Albania', 'Algeria', 'Angola', ..., 'Venezuela', 'Western Sahara', 'Zambia', 'Zimbabwe']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [ \"list of columsn that you want to convert\"]\n",
    "# df[cols] = df[cols].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Efficient File Formats\n",
    "\n",
    "- Parquet: A columnar storage file format that is highly efficient for both read and write operations.\n",
    "- Feather: Optimized for speed, especially for data exchange between pandas and R.\n",
    "- HDF5: Suitable for storing large numerical data.\n",
    "\n",
    "Use Libraries for distributed computing\n",
    "\n",
    "- Spark: Use PySpark for distributed data processing.\n",
    "- Dask: Parallelize computations on a single machine or across a cluster.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
